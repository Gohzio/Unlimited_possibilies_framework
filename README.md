# Narrative Engine â€“ Roadmap & Architecture

This project is a deterministic narrative RPG engine where **the engine owns truth**  
and an external LLM acts purely as a constrained narrator.

The LLM is *not authoritative*.  
All state changes are validated, applied, or rejected by the engine.

---

## âœ… Locked Architectural Decisions

- The engine **does not load models**
- LLM inference runs **externally**
- Communication uses an **OpenAI-compatible HTTP API**
- **LM Studio** is the primary supported runtime
- A **single LLM** is sufficient (engine enforces constraints)

---

## ğŸ§­ Current Development Roadmap

### 1ï¸âƒ£ Lock LLM Runtime & API
- [x] Use **LM Studio** as the reference implementation
- [x] OpenAI-compatible `/v1/chat/completions` schema
- [x] HTTP-based, replaceable backend (LM Studio / Ollama / OpenAI)
- [x] No `.gguf` loading inside the engine

---

### 2ï¸âƒ£ Complete Character / World JSON Structure
- [x] Lock `WorldDefinition` schema
- [x] Five sections:
  - Meta
  - World
  - Narrator
  - Constraints
  - Output
- [x] Player-editable via UI **or** JSON upload
- [x] Engine treats this as authoritative configuration

---

### 3ï¸âƒ£ Define LLMRequest + LLMResponse
- [ ] Define engine-facing request struct
- [ ] Include:
  - Prompt text
  - Model name
  - Temperature / top-p (later)
- [ ] Define response struct:
  - Raw text
  - Finish reason
  - Token usage (optional)
- [ ] Keep interface backend-agnostic

---

### 4ï¸âƒ£ ğŸ§± Prompt Builder (WorldDefinition â†’ Prompt)
- [ ] Render `WorldDefinition` into deterministic system prompt
- [ ] Inject:
  - World rules
  - Narrator role
  - Style guidelines
  - Hard constraints (`must_not`, `must_always`)
- [ ] Append:
  - Current world state snapshot
  - Recent message history
  - Player input
- [ ] Explicit output rules (machine-readable)

---

### 5ï¸âƒ£ ğŸ” Output Parser + Validator
- [ ] Split narration vs events
- [ ] Parse structured event output (JSON)
- [ ] Validate:
  - Schema correctness
  - Stat existence
  - Rule violations
- [ ] Reject / defer invalid events
- [ ] Never trust raw LLM output

---

### 6ï¸âƒ£ ğŸ”„ Hook Into `EngineCommand::UserInput`
- [ ] On user input:
  - Build prompt
  - Send LLM request
  - Receive response
- [ ] Parse output
- [ ] Apply validated events
- [ ] Emit:
  - Renderable narration
  - System feedback for rejected actions

---

### 7ï¸âƒ£ ğŸ§ª Test With a Dummy Model
- [ ] Stub LLM client returning fixed responses
- [ ] Test:
  - Happy path
  - Invalid JSON
  - Rule-breaking events
- [ ] Ensure engine never panics on bad output
- [ ] Confirm UI rendering works without live inference

---

## ğŸ§  Core Philosophy

> **The engine is law.  
> The LLM is a storyteller.  
> The player is always in control.**

This design ensures:
- Deterministic gameplay
- Replaceable AI backends
- Strong modding potential
- No model lock-in
- Long-term maintainability

---

## ğŸš€ Future (Not Yet Implemented)

- Multi-character narrator styles
- Streaming token support
- Per-world output formatting
- Advanced prompt debugging tools
- Saveable prompt presets

---

*This README reflects locked decisions.  
Changes to these principles should be deliberate and documented.*

| Section                | Player             | Engine    | LLM  |
| ---------------------- | -----------------  | --------  | ---  |
| Meta                   | âœ… edit            | âŒ        | âŒ   |
| Identity               | âœ… edit (pre-game) | âŒ        | âŒ   |
| Mechanical Foundations | âœ… start           | âœ… mutate | âŒ   |
| Equipment / Inventory  | âœ… start           | âœ… mutate | âŒ   |
| Narrative Directives   | âœ… edit anytime    | âŒ        | âŒ   |
